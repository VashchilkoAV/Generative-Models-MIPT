{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1. Likelihood-based models\n",
    "\n",
    "- **Task 1 (5 points): Warmup**\n",
    "- Task 2 (10 points): PixelCNN\n",
    "- Task 3 (10 points): Conditional PixelCNN\n",
    "- Task 4 (10 points): RealNVP\n",
    "- \\* Bonus (10+++ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup task\n",
    "\n",
    "In this task we will play with simplest likelihood-based models with both 1D and 2D data. The task consists of 2 parts:\n",
    "- Likelihood model in 1D - fitting histogram using SGD\n",
    "- Deep Autoregressive model in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Fitting histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will build our first likelihood-based model for 1D data and will try to fit it using gradient methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose your device: don't forget to switch to GPU runtime when working in colab with cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the procedure of data generation. It will generate a dataset of samples $x \\in \\{0 \\dots 99\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data():\n",
    "    count = 10000\n",
    "    rand = np.random.RandomState(0)\n",
    "    a = 0.3 + 0.1 * rand.randn(count)\n",
    "    b = 0.8 + 0.05 * rand.randn(count)\n",
    "    mask = rand.rand(count) < 0.5\n",
    "    samples = np.clip(a * mask + b * (1 - mask), 0.0, 1.0)\n",
    "    \n",
    "    return np.digitize(samples, np.linspace(0.0, 1.0, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate data and perform train/val/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sample_data()\n",
    "train_data, test_data = train_test_split(data, test_size = 0.3)\n",
    "train_data, val_data = train_test_split(train_data, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot and visualize the histogram of training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data):\n",
    "    counts = Counter(data)\n",
    "    keys = list(counts.keys())\n",
    "    values = list(counts.values())\n",
    "    plt.bar(keys, values)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lecture we have discussed how to build histogram model. But this model is not the best choice for high-dimensional data. So, we suggest you to implement the following parametrized model:\n",
    "\n",
    "$$ p_\\theta(x)_i = \\frac{e^{\\theta_i}}{\\sum_j{e^{\\theta_j}}} $$\n",
    "\n",
    "Where $\\theta=(\\theta_0 \\dots \\theta_{99})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose you to implement this model in the following class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleProbabilityModel(nn.Module):\n",
    "    # Store all parameters of your model as class fields in constructor\n",
    "    def __init__(self,  num_elements=100):\n",
    "        super(SimpleProbabilityModel, self).__init__()\n",
    "        \n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "        \n",
    "    # Forward should return vector of log probabilities for each element\n",
    "    def forward(self):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "    \n",
    "    # Should sample element using probabilities, obtained from parameters. Return single number 0..99\n",
    "    def sample(self):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train this model using negative log-likelihood optimization: $ L_i = -\\log p_{y_i} $. Implement this loss calculation for your model given a batch of data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: n.array of numbers from your training distribution\n",
    "# model: instance of your SimpleProbabilityModel.\n",
    "# should return: negative log-likelihood of your data given the model to perform backpropagation\n",
    "def calc_loss(data, model):\n",
    "    ################\n",
    "    # YOUR CODE HERE\n",
    "    ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create instance of our model and perform training. Note that if you calculated previous loss as classic natural logarithm, here we scale it to binary logarithm for logging likelihood in bits (which is better for interpretation and comparisons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleProbabilityModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple_model(model, train_data, val_data, num_epochs=10000, batch_size=4000, lr=0.01):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for i in range(num_epochs):\n",
    "        for j in range(len(train_data) // batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            batch = train_data[batch_size * j:batch_size * (j + 1)]\n",
    "            l = calc_loss(batch, model)\n",
    "            train_losses.append(l.item() / math.log(2))\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        l = calc_loss(val_data, model)\n",
    "        val_losses.append(l.item() / math.log(2))\n",
    "    \n",
    "    print(\"Train NLL(bits)\")\n",
    "    plt.plot(train_losses, color='green')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Val NLL(bits)\")\n",
    "    plt.plot(val_losses, color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Final validation NLL(bits): {}\".format(val_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_simple_model(model, train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also tune your training parameters (number of epochs, batch size, learning rate, optimizer), to improve validation NLL. You should obtain something below 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's sample values from our model and visualize histograms of our test data and our sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = [model.sample() for _ in range(len(test_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(sampled_data)\n",
    "plot_histogram(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training here might not yield perfect results, but pictures should look at least similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2. 2D discrete data. Autoregressive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will build our own autoregressive model to work with two-dimensional discrete data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load 2D distribution as is from file. It's a 200x200 numpy array with probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab users: download file\n",
    "! wget https://github.com/egiby/Generative-Models-MIPT/raw/main/module1-likelihood/distribution.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_distribution = np.load(\"distribution.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define class to sample pair of numbers $(x,y) \\in \\{0 \\dots 199\\}^2$ from this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleDist:\n",
    "    def __init__(self, distribution):\n",
    "        self.probabilities = distribution.flatten()\n",
    "        self.rows, self.cols = distribution.shape\n",
    "        self.values = np.array([[i // self.cols, i % self.cols] for i in range(self.rows * self.cols)])\n",
    "\n",
    "    def sample(self):\n",
    "        idx = np.random.choice(self.rows * self.cols, p = self.probabilities)\n",
    "        \n",
    "        return self.values[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we define distribution, sample data and create train/val/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2D = SampleDist(original_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 100000\n",
    "sampled_data = np.array([dist2D.sample() for _ in range(SIZE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(sampled_data, test_size = 0.2)\n",
    "train_data, val_data = train_test_split(train_data, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build our autoregressive model in $(x, y)$ as follows:\n",
    "\n",
    "- Train marginal model $p(x)$ as in part 1\n",
    "- Create and train conditional model $p(y|x)$ as multi-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, create class for your conditional model $p(y|x)$. It should take $x$ as batch of integer inputs and return batch of probability distributions over $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalModel(nn.Module):\n",
    "    # Store all your trainable layers as model fiels in constuctor\n",
    "    def __init__(self):\n",
    "        super(ConditionalModel, self).__init__()\n",
    "        \n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "    \n",
    "    # Forward pass takes LongTensor x of shape (N,) and should return predicted logprobs of shape (N, 200)\n",
    "    def forward(self, x):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_model = ConditionalModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cond_model(cond_model, train_data, num_epochs=100, lr=0.001, batch_size=10000):\n",
    "    dataset = torch.utils.data.TensorDataset(torch.LongTensor(train_data.T[0]).to(device), \n",
    "                                             torch.LongTensor(train_data.T[1]).to(device))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    loss = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(cond_model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        for X_train, Y_train in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = cond_model(X_train)\n",
    "            l = loss(predictions, Y_train)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(l.item() / math.log(2))\n",
    "    \n",
    "    print(\"Train NLL(bits)\")\n",
    "    plt.plot(train_losses, color='green')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cond_model(cond_model, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build compound model, we will also need our simple model from part 1, trained on marginal data from our distribution (only x values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model = SimpleProbabilityModel(num_elements=200).to(device)\n",
    "train_simple_model(x_model, train_data.T[0], val_data.T[0], num_epochs=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to build compound model for our total $(x, y)$ distribution modeling. Having two trained models implement sampling procedure and probability calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompoundModel:\n",
    "    def __init__(self, x_model, cond_model):\n",
    "        self.x_model = x_model\n",
    "        self.cond_model = cond_model\n",
    "        \n",
    "        self.x_model.eval()\n",
    "        self.cond_model.eval()\n",
    "    \n",
    "    # Given two numbers x, y from 0 .. 199, return NLL value -log p(x,y)\n",
    "    # Normalize in in the way it will return NLL in bits / dimention (binary log divided by two in our case)\n",
    "    def get_logprob(self, x, y):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "    \n",
    "    # Implement sampling procedure. One call should return sample (x, y) as numpy array from two elements\n",
    "    def sample(self):\n",
    "        ################\n",
    "        # YOUR CODE HERE\n",
    "        ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_model = CompoundModel(x_model, cond_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate total average NLL in bits / dimension on your validation data. Tune training parameters and conditional model architecture to boost your performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_logprob = 0\n",
    "for elem in val_data:\n",
    "    logprob = compound_model.get_logprob(elem[0], elem[1])\n",
    "    total_logprob += logprob\n",
    "print(\"Total NLL on validation data per dimension: {}\".format(total_logprob / val_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if sampling from your model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_model.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, get enough samples from your final model and display 2D histogram of the results. Compare them with the results you can get from your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_2d_data = np.array([compound_model.sample() for _ in range(test_data.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2dhistogram(data):\n",
    "    plt.hist2d(data.T[0], data.T[1], bins=200, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_2dhistogram(sampled_2d_data)\n",
    "plot_2dhistogram(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't this picture resemble anything? (look at the rotated version of the histogram). Check out how your original distribution looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(original_distribution, cmap=\"gray\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
