{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "square-longer",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ed3174",
   "metadata": {},
   "source": [
    "## VAE with AF Prior (10 points)\n",
    "In this part, implement a VAE with an Autoregressive Flow prior ([VLAE](https://arxiv.org/abs/1611.02731)) with the following characteristics:\n",
    "\n",
    "*   16-dim latent variables $z$ with a MADE prior, with $\\epsilon \\sim N(0, I)$\n",
    "*   An approximate posterior $q_\\theta(z|x) = N(z; \\mu_\\theta(x), \\Sigma_\\theta(x))$, where $\\mu_\\theta(x)$ is the mean vector, and $\\Sigma_\\theta(x)$ is a diagonal covariance matrix\n",
    "*   A decoder $p(x|z) = N(x; \\mu_\\phi(z), I)$, where $\\mu_\\phi(z)$ is the mean vector. (We are not learning the covariance of the decoder)\n",
    "\n",
    "You can use the same encoder / decoder architectures and training hyperparameters as part (a). For your MADE prior, it would suffice to use two hidden layers of size $512$. More explicitly, your MADE AF (mapping from $z\\rightarrow \\epsilon$) should output location $\\mu_\\psi(z)$ and scale parameters $\\sigma_\\psi(z)$ and do the following transformation on $z$:\n",
    "$$\\epsilon = z \\odot \\sigma_\\psi(z) + \\mu_\\psi(z)$$\n",
    "\n",
    "where the $i$th element of $\\sigma_\\psi(z)$ is computed from $z_{<i}$ (same for $\\mu_\\psi(z)$) and optimize the objective\n",
    "\n",
    "$$-E_{z\\sim q(z|x)}[\\log{p(x|z)}] + E_{z\\sim q(z|x)}[\\log{q(z|x)} - \\log{p(z)}]$$\n",
    "where $$\\log{p(z)} = \\log{p(\\epsilon)} + \\log{\\det\\left|\\frac{d\\epsilon}{dz}\\right|}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**You will provide the following deliverables**\n",
    "\n",
    "\n",
    "1.   Over the course of training, record the average full negative ELBO, reconstruction loss, and KL term of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2.   Report the final test set performance of your final model\n",
    "3. 100 samples from your trained VAE\n",
    "4. 50 real-image / reconstruction pairs (for some $x$, encode and then decode)\n",
    "5. Interpolations of length 10 between 10 pairs of test images from your VAE (100 images total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-proposal",
   "metadata": {},
   "source": [
    "## PixelVAE (10 points)\n",
    "Implement and train a VAE with a PixelCNN decoder, and get it to produce good samples but not ignore latents. It may help to reference the latent variable model slides on techniques to prevent posterior collapse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
